# 大数据处理
# 内容
  大数据处理一般情况下不要求手写代码，更重要的是叙述出来思路和方法，主要的解题思路有以下两个：
  
  - 方法：
    
    - 看能否用特殊的数据结构解决（位图、布隆过滤器、堆（TOP K 问题））
    
    - 哈希切分
  
  当面试官问到这一类的题目的时候，首先考虑内存足够的情况（排序、查找等手段来进行处理），内存不够则采用下面套路进行回答。
  
下面提供了10个题，看完了就能明白大数据问题是如何处理，以及如何进行回答的了。（都是套路问题）

# 题目
### 1、给一个超过100G大小的log file，log中存着ip地址，设计算法找到出现次数最多的ip地址？

- 解决方法：
  
  哈希切分。将100G的文件分成1000份，通过同一个散列函数映射到相应的文件，此时同一个ip一定会映射到同一个文件，利用map进行存储，其他文件中的ip只需调用map[]进行插入，最后map中value最大的对应的key就是出现次数最多的ip。

### 2、与上题相同，如何找到top K的ip？如何直接用Linux命令实现？

- 解决方法：
  
  本题采用哈希切分，统计每个ip出现的次数，然后利用最小堆把每个小文件录入，遍历每个小文件内IP出现的次数和它对比即可。 

- Linux指令为：
  
  > 待补充

### ⒊ 给定100亿整数，设计算法找到只出现一次的整数？
- 解决方法一：
  
  将100亿整数切分成100份，平均每份大约400M左右，将每一份加载到内存中，利用哈希表查找只出现一次的数，最后将100份的结构汇总查找。

- 解决方法二：采用位图。

  扩展问题：找出出现1、2或者多次出现的整数，这时如果我们用一个bit位表示状态显然是不够的，因此我们可以用两个比特位表示：00表示没出现，01表示出现1次，10表示出现2次，11表示出现多次，这样便可区分一次和多次。


### 4、给两个文件，分别有100亿整数，我们只有1G内存，如何找到两个文件的交集？

- 解决方案一：
  
  将一个文件切割成100份，分别将每一份加载到内存中，然后用第二个文件中的数据到每一份中查找，时间复杂度为(O(n^2))。

- 解决方案二：
  
  哈希切分。具体步骤如上图。通过一个散列函数，将相同的数映射到同一个文件中，并对文件进行编号。如果两个文件存在交集，一定会在编号相同的文件中产生交集，这时候只需要将他们编号相同的文件比较即可，时间复杂度为(O(N))。

- 解决方案三：
  
  位图。将一个文件中的数据映射到位图中，然后再用第二个文件中的数据到位图中查找。时间复杂度为(O(N))。

### ⒌、1个文件有100亿个int，1G内存，设计算法找到出现次数不超过2次的所有整数？
- 解决方法：
  位图。采用方法如第三题的扩展。

### ⒍、给两个文件，分别有100亿个query（查询），我们只有1G内存，如何找到两个文件交集？
  100亿个字节大概是10G内存，假设每个query是50个bytes，所以100亿个query大概是500G内存。

- 解决方法一：
  
  哈希切分。与第四题的方法一致，只需要将query转换成int，然后再经过散列函数映射到文件中。切分5000份。

- 解决方法二：

  布隆过滤器。将其中一个文件的内存映射到位图中，在用另一个文件中的内存到这个位图中寻找。
  
### ⒎ 如何扩展BloomFilter使得它支持删除元素的操作？
  
  由于布隆过滤器是不存在为一定，存在不一定，如果很多个元素拥有相同的BIT位，此时删除其中一个位，会影响到其他的数据，所以不能删除，因此可以采用对每个位进行引用计数即可。

### ⒏ 如何扩展BloomFilter使得它支持引用计数操作？
  
  用一个vector，将集合中的每个元素经过多个散列函数映射到多个位置，而每个位置的值则表示有多少个元素映射到这个位置。
  ```
  void Set(const K& key)
  {
    index1=
    _bitMap[index]++;
  
    index2=
    _bitMap[index2]++;
  
    index3=
    _bitMap[index3]++;
  }
    
  vector<size_t> count;
```

### ⒐ 给上千个文件，每个文件大小为1K-100M，给n个词，设计算法对每个词找到所有包含它的文件，你只有100K内存？

（倒排索引思想，比如在浏览器中搜索python显示的内容，网页会给出所有的python相关的内容，同时python变成高亮。百度网页是从爬虫来的，在搜索中采用分词）


一个文件info 准备用来保存ｎ个词和包含其的文件信息。

首先把ｎ个词分成ｘ份。对每一份用生成一个布隆过滤器（因为对ｎ个词只生成一个布隆过滤器，内存可能不够用）。把生成的所有布隆过滤器存入外存的一个文件Filter中。

将内存分为两块缓冲区，一块用于每次读入一个布隆过滤器，一个用于读文件(读文件这个缓冲区使用相当于有界生产者消费者问题模型来实现同步)，大文件可以分为更小的文件，但需要存储大文件的标示信息（如这个小文件是哪个大文件的）。

对读入的每一个单词用内存中的布隆过滤器来判断是否包含这个值，如果不包含，从Filter文件中读取下一个布隆过滤器到内存，直到包含或遍历完所有布隆过滤器。如果包含,更新info 文件。直到处理完所有数据。删除Filter文件。


### ⒑ 有一个词典，包含N个英文单词，现在任意给一个字符串，设计算法找出包含这个字符串的所有英文单词？

本题需要借助一种特殊的数据结构—字典树。又称单词查找树、Trie树，是一种哈稀树的变种。

它的应用有：用于统计、排序和保存大量的字符串，经常被搜索引擎系统用作文本词频统计。

- 优点：利用字符串的公共前缀减少查询时间，最大限度地减少无畏的字符串比较，查询效率高于哈希表。

- 缺点：每一个节点都开辟了26个字母的空间，而不管是否使用，浪费空间。

- 基本性质：
  
  根节点不包含字符，除根节点外每个节点都只包含一个字符。从根节点到某一节点，路径上所有经过的字符连接起来，为该节点对应的字符串。每个节点的所有子节点包含的字符都不相同。

- 解决方案：
  
  用给出的N个单词简历一棵与上述字典树不同的字典树，用任意字符串与字典树中的每个节点中的单词进行比较，在每层中查找与任意字符串首字母一样的，找到则遍历其下面的子树，直到全部相同。
